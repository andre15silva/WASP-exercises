{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "db341436",
   "metadata": {},
   "source": [
    "## Assignment 2\n",
    "\n",
    "In the second assignment, we are going to use a large language model in a retrieval-augmented setup. As an application, we are going to consider a question answering task.\n",
    "\n",
    "You can use any LLMs you want in this assignment, but your solution must consider at least one open model (e.g. Mistral or one of the Llama models). Optionally, you may compare to a commercial model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33f1835b",
   "metadata": {},
   "source": [
    "The dataset we will use in this assignment is a simplified version of Natural Questions, which was compiled by Google and consists of real search engine queries about factual questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c064641b-504f-4d74-aaaa-cb0020c51327",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>gold_context</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>what purpose did seasonal monsoon winds have o...</td>\n",
       "      <td>enabled European empire expansion into the Ame...</td>\n",
       "      <td>The westerlies (blue arrows) and trade winds (...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>who got the first nobel prize in physics</td>\n",
       "      <td>Wilhelm Conrad Röntgen, of Germany</td>\n",
       "      <td>The award is presented in Stockholm at an annu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>when is the next deadpool movie being released</td>\n",
       "      <td>May 18, 2018</td>\n",
       "      <td>Though the original creative team of Reynolds,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>where did the idea of fortnite come from</td>\n",
       "      <td>as a cross between Minecraft and Left 4 Dead</td>\n",
       "      <td>Fortnite is set in contemporary Earth, where t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>which mode is used for short wave broadcast se...</td>\n",
       "      <td>MFSK Olivia</td>\n",
       "      <td>All one needs is a pair of transceivers, each ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4284</th>\n",
       "      <td>who challenged the aristotelian model of a geo...</td>\n",
       "      <td>Copernicus</td>\n",
       "      <td>Planets Variations in speed through the zodiac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4285</th>\n",
       "      <td>when was the miraculous journey of edward tula...</td>\n",
       "      <td>March 30, 2006</td>\n",
       "      <td>The Miraculous Journey of Edward Tulane-wikipe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4286</th>\n",
       "      <td>character in macbeth who is murdered and appea...</td>\n",
       "      <td>Lord Banquo</td>\n",
       "      <td>Banquo Thane of Lochaber Macbeth character Thé...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4287</th>\n",
       "      <td>when was as you like it first performed</td>\n",
       "      <td>uncertain, though a performance at Wilton Hous...</td>\n",
       "      <td>As You Like It-wikipedia As You Like It Jump t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4288</th>\n",
       "      <td>when did computer become widespread in homes a...</td>\n",
       "      <td>1980s</td>\n",
       "      <td>Children playing Paperboy on an Amstrad CPC 46...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4289 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               question  \\\n",
       "0     what purpose did seasonal monsoon winds have o...   \n",
       "1              who got the first nobel prize in physics   \n",
       "2        when is the next deadpool movie being released   \n",
       "3              where did the idea of fortnite come from   \n",
       "4     which mode is used for short wave broadcast se...   \n",
       "...                                                 ...   \n",
       "4284  who challenged the aristotelian model of a geo...   \n",
       "4285  when was the miraculous journey of edward tula...   \n",
       "4286  character in macbeth who is murdered and appea...   \n",
       "4287            when was as you like it first performed   \n",
       "4288  when did computer become widespread in homes a...   \n",
       "\n",
       "                                                 answer  \\\n",
       "0     enabled European empire expansion into the Ame...   \n",
       "1                    Wilhelm Conrad Röntgen, of Germany   \n",
       "2                                          May 18, 2018   \n",
       "3          as a cross between Minecraft and Left 4 Dead   \n",
       "4                                           MFSK Olivia   \n",
       "...                                                 ...   \n",
       "4284                                         Copernicus   \n",
       "4285                                     March 30, 2006   \n",
       "4286                                        Lord Banquo   \n",
       "4287  uncertain, though a performance at Wilton Hous...   \n",
       "4288                                              1980s   \n",
       "\n",
       "                                           gold_context  \n",
       "0     The westerlies (blue arrows) and trade winds (...  \n",
       "1     The award is presented in Stockholm at an annu...  \n",
       "2     Though the original creative team of Reynolds,...  \n",
       "3     Fortnite is set in contemporary Earth, where t...  \n",
       "4     All one needs is a pair of transceivers, each ...  \n",
       "...                                                 ...  \n",
       "4284  Planets Variations in speed through the zodiac...  \n",
       "4285  The Miraculous Journey of Edward Tulane-wikipe...  \n",
       "4286  Banquo Thane of Lochaber Macbeth character Thé...  \n",
       "4287  As You Like It-wikipedia As You Like It Jump t...  \n",
       "4288  Children playing Paperboy on an Amstrad CPC 46...  \n",
       "\n",
       "[4289 rows x 3 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "nq_data = pd.read_csv('nq_simplified.val.tsv', sep='\\t', header=None, names=['question', 'answer', 'gold_context'], quoting=3)\n",
    "nq_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01074d32",
   "metadata": {},
   "source": [
    "### Step 1: Evaluating an LLM on Natural Questions\n",
    "\n",
    "Load an LLM and explore different prompting strategies to try to make it answer the questions in the dataset. As a benchmark, you can use the ROUGE-1 precision/recall/F1 scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bf952eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rouge1(gold, predicted):\n",
    "  assert(len(gold) == len(predicted))\n",
    "  n_p = 0\n",
    "  n_g = 0\n",
    "  n_c = 0\n",
    "  for g, p in zip(gold, predicted):\n",
    "    g = set(cleanup(g).strip().split())\n",
    "    p = set(cleanup(p).strip().split())\n",
    "    n_g += len(g)\n",
    "    n_p += len(p)\n",
    "    n_c += len(p.intersection(g))\n",
    "  pr = n_c / n_p\n",
    "  re = n_c / n_g\n",
    "  if pr > 0 and re > 0:\n",
    "    f1 = 2*pr*re/(pr + re)\n",
    "  else:\n",
    "    f1 = 0.0\n",
    "  return pr, re, f1\n",
    "\n",
    "def cleanup(text):\n",
    "  text = text.replace(',', ' ')\n",
    "  text = text.replace('.', ' ')\n",
    "  return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d742dad1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/x_andaf/WASP-exercises/WASP DL4NLP 2024/Assignment 2/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "class Mistral:\n",
    "    def __init__(self):\n",
    "        self.pipe = pipeline(\"text-generation\", model=\"mistralai/Mistral-7B-Instruct-v0.3\", max_new_tokens=256, device=\"cuda\")\n",
    "\n",
    "    def prompt(self, questions):\n",
    "        messages = [[{\"role\": \"user\", \"content\": question},] for question in questions]\n",
    "        messages = self.pipe(messages)\n",
    "        return messages\n",
    "\n",
    "    def simple_prompt(self, questions):\n",
    "        messages = [[{\"role\": \"user\", \"content\": f\"Answer the following question with a short and straightforward answer: {question}\"},] for question in questions]\n",
    "        messages = self.pipe(messages)\n",
    "        return messages\n",
    "\n",
    "    def context_prompt(self, questions, contexts):\n",
    "        messages = [[{\"role\": \"user\", \"content\": f\"Context: {context}\\n\\nAnswer the following question with a short and straightforward answer based on the provided context: {question}\"},] for question, context in zip(questions, contexts)]\n",
    "        messages = self.pipe(messages)\n",
    "        return messages   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3bdfeaed-0b26-45be-9480-81a2986fbf37",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:04<00:00,  1.59s/it]\n"
     ]
    }
   ],
   "source": [
    "mistral = Mistral()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "16239376",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.4470588235294118, 0.03881511746680286, 0.07142857142857144)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answers = mistral.prompt(nq_data[\"question\"][:16])\n",
    "extracted_answers = [answer[-1][\"generated_text\"][-1][\"content\"] for answer in answers]\n",
    "rouge1(extracted_answers, nq_data[\"answer\"][:16])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2c6131e7-4f20-4928-9dbb-f3c547d637e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.3764705882352941, 0.05786618444846293, 0.10031347962382445)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simple_answers = mistral.simple_prompt(nq_data[\"question\"][:16])\n",
    "extracted_simple_answers = [answer[-1][\"generated_text\"][-1][\"content\"] for answer in simple_answers]\n",
    "rouge1(extracted_simple_answers, nq_data[\"answer\"][:16])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "161c11ce-aa73-455d-b22c-9c4fdbdc3217",
   "metadata": {},
   "source": [
    "### Step 2: An idealized retrieval-augmented LLM\n",
    "\n",
    "The third column in the dataset (called gold_context above) contains a text fragment from a Wikipedia page, from which the answer can be deduced. Try out new prompts where you include this relevant context. How does this change the evaluation scores?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e1799d2e-3d1c-4a46-9d89-7563fd40e551",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.6, 0.100990099009901, 0.17288135593220338)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gold_context_answers = mistral.context_prompt(nq_data[\"question\"][:16], nq_data[\"gold_context\"][:16])\n",
    "extracted_gold_context_answers = [answer[-1][\"generated_text\"][-1][\"content\"] for answer in gold_context_answers]\n",
    "rouge1(extracted_gold_context_answers, nq_data[\"answer\"][:16])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87876df3-895b-496c-a189-86ca10624642",
   "metadata": {},
   "source": [
    "### Step 3: Setting up the retriever\n",
    "\n",
    "The setup in Step 2 is idealized, because we provided a context from Wikipedia where we know that the answer is avaialable. In real-world settings, this is not going to be the case.\n",
    "\n",
    "To make this assignment work in Colab, we are going to work with a rather small set of passages. You can download these texts from here. For a given question, we are going to search among these passages to find the best-matching passage."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1216096-c9d0-4025-8633-c839edfbace6",
   "metadata": {},
   "source": [
    "\n",
    "#### Representing the passages as vectors\n",
    "\n",
    "Set up a representation model that maps a text passage to a numerical vector.\n",
    "\n",
    "For instance, some model from SentenceTransformers, such as all-MiniLM-L6-v2 could be a good choice.\n",
    "\n",
    "Apply this model to all text passages.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "05197d2c-4e4f-47cd-afab-bf15434830cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "and fielded .964. He was 10th in Hoofdklasse in average, fifth with 47 hits (one behind brother Mark), 6th with 24 RBI and tied for 7th with 9 steals. He led the 2005 European Championship with 14 walks in 10 contests; no one else had more than 9. He also played error-free ball at second base. In the 2005 Baseball World Cup, Duursma hit .302/.412/.395 with 12 runs in 11 games. During the 2006 World Baseball Classic, Duursma had the best average on the Dutch team, though he played in just one game. He went 2 for 4 with two\n",
      "\n",
      "34312\n"
     ]
    }
   ],
   "source": [
    "with open(\"passages.txt\", \"r\") as f:\n",
    "    passages = [passage for passage in f.readlines()]\n",
    "\n",
    "print(passages[0])\n",
    "print(len(passages))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d6f18dc6-6313-40bc-a28e-fec00c99b988",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "embeddings = model.encode(passages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "213af6a8-6c9f-4dc8-a10b-c520a47b0a8b",
   "metadata": {},
   "source": [
    "#### Storing the passage vectors in a database\n",
    "\n",
    "We now create a vector database that allows us to search efficiently for the neareast neighbors in the vector space of a given query vector. We recommend the FAISS library for this purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a0fdbea6-4149-4c1a-9519-abc8a5562d16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "\n",
    "index = faiss.IndexFlatL2(embeddings.shape[1])\n",
    "index.add(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7581552b-6f19-4ee0-a102-45ad5b1b702a",
   "metadata": {},
   "outputs": [],
   "source": [
    "question_embeddings = model.encode(nq_data[\"question\"][:16])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "4660de83-fe64-4671-905d-4946114fd59f",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, ix = index.search(question_embeddings, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "0f3b8961-e128-4e35-8afd-30fbe6074e55",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.27058823529411763, 0.05263157894736842, 0.08812260536398467)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_context_answers = mistral.golden_context_prompt(nq_data[\"question\"][:16], [passages[idx[0]] for idx in ix])\n",
    "extracted_rag_context_answers = [answer[-1][\"generated_text\"][-1][\"content\"] for answer in rag_context_answers]\n",
    "rouge1(extracted_rag_context_answers, nq_data[\"answer\"][:16])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3693eaa4-3f54-4939-bcc9-6099b7443540",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
