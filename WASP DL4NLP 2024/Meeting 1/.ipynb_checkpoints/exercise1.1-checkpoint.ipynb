{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yXVM2mFqUl5C"
   },
   "source": [
    "# Exercise 1-1: Preprocessing and text classification.\n",
    "\n",
    "Most of your work here is intended to make sure you understand the *practical* side of working with PyTorch for NLP tasks. We'll also look at simple neural architectures for classification.\n",
    "\n",
    "Most of your work will be the implementation of\n",
    "- *preprocessing* utlities to convert the text into a numerical format that can be used by PyTorch,\n",
    "- the *training loop* that takes an untrained model, applies it to a training set, and updates the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B9mxgr0hd6ZS"
   },
   "source": [
    "# Preliminaries\n",
    "\n",
    "To run the code, the following libraries need to be installed on your machine:\n",
    "\n",
    "- [PyTorch](https://pytorch.org/) is the machine learning library we will use. You can see on the PyTorch home page how to install the library.\n",
    "- [scikit-learn](https://scikit-learn.org/) for a couple of minor utility functions.\n",
    "- [spaCy](https://spacy.io/) for basic linguistic preprocessing.\n",
    "- [pandas](https://pandas.pydata.org/) to read the files.\n",
    "- [tqdm](https://tqdm.github.io/) for a progress bar used in the training loop.\n",
    "- [NumPy](https://numpy.org/) to combine some matrices in the final part of the assignment.\n",
    "\n",
    "If you use Colab, nothing needs to be installed since all libraries are included in the standard setup.\n",
    "\n",
    "PyTorch is mandatory for this assignment, but the other libraries are simply for convenience and you can solve the assignment without them (if for some reason you don't want to install the libraries)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y7wpMbX6Ul5X"
   },
   "source": [
    "Download the training and test files from [this directory](http://www.cse.chalmers.se/~richajo/dat450/assignments/data/). Place them in some directory where this notebook can access them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "Aea8GoZNUl5Z"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-04-11 15:31:14--  http://www.cse.chalmers.se/~richajo/waspnlp2024/dredze_amazon_reviews.zip\n",
      "Resolving www.cse.chalmers.se (www.cse.chalmers.se)... 129.16.222.93\n",
      "Connecting to www.cse.chalmers.se (www.cse.chalmers.se)|129.16.222.93|:80... connected.\n",
      "HTTP request sent, awaiting response... 301 Moved Permanently\n",
      "Location: https://www.cse.chalmers.se/~richajo/waspnlp2024/dredze_amazon_reviews.zip [following]\n",
      "--2024-04-11 15:31:14--  https://www.cse.chalmers.se/~richajo/waspnlp2024/dredze_amazon_reviews.zip\n",
      "Loaded CA certificate '/etc/ssl/certs/ca-certificates.crt'\n",
      "Connecting to www.cse.chalmers.se (www.cse.chalmers.se)|129.16.222.93|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 3431912 (3.3M) [application/zip]\n",
      "Saving to: ‘dredze_amazon_reviews.zip’\n",
      "\n",
      "dredze_amazon_revie 100%[===================>]   3.27M  4.76MB/s    in 0.7s    \n",
      "\n",
      "2024-04-11 15:31:15 (4.76 MB/s) - ‘dredze_amazon_reviews.zip’ saved [3431912/3431912]\n",
      "\n",
      "Archive:  dredze_amazon_reviews.zip\n",
      "  inflating: dredze_amazon_reviews.tsv  \n"
     ]
    }
   ],
   "source": [
    "!wget http://www.cse.chalmers.se/~richajo/waspnlp2024/dredze_amazon_reviews.zip\n",
    "!unzip dredze_amazon_reviews.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c9Lhhz7ZUl5h"
   },
   "source": [
    "### Reading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "Ip8n3V-nUl5i"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K8Fm1AXuUl5j"
   },
   "source": [
    "We use Pandas to read the file containing the data. It consists of three tab-separated columns, where the first and second columns contain the gold-standard labels (product type and sentiment labels) and the third contains the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "WC44--mWUl5j"
   },
   "outputs": [],
   "source": [
    "amazon_corpus = pd.read_csv('dredze_amazon_reviews.tsv', sep='\\t', header=None, names=['product', 'sentiment', 'text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bRtxL0WXUl5k"
   },
   "source": [
    "Here, we can see the first few instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "RAWphWWpUl5m"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>music</td>\n",
       "      <td>neg</td>\n",
       "      <td>i bought this album because i loved the title ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>music</td>\n",
       "      <td>neg</td>\n",
       "      <td>i was misled and thought i was buying the enti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>books</td>\n",
       "      <td>neg</td>\n",
       "      <td>i have introduced many of my ell , high school...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>books</td>\n",
       "      <td>pos</td>\n",
       "      <td>anything you purchase in the left behind serie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>dvd</td>\n",
       "      <td>pos</td>\n",
       "      <td>i loved these movies , and i cant wiat for the...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  product sentiment                                               text\n",
       "0   music       neg  i bought this album because i loved the title ...\n",
       "1   music       neg  i was misled and thought i was buying the enti...\n",
       "2   books       neg  i have introduced many of my ell , high school...\n",
       "3   books       pos  anything you purchase in the left behind serie...\n",
       "4     dvd       pos  i loved these movies , and i cant wiat for the..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "amazon_corpus.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FWKo8UalUl5n"
   },
   "source": [
    "We split the data into a training (80%) and a validation part (20%). We use the convenience function `train_test_split` from scikit-learn.\n",
    "\n",
    "Following standard notation, we refer to the input part of the data (that is, the documents) as `X` and the output part (classification labels) as `Y`. Initially, we are going to use the *sentiment* label as the target we want to learn to predict. (We are going to use the product category at the end of the exercise.)\n",
    "\n",
    "The validation will be used to compute diagnostic scores during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "upAnPIKJUl5q"
   },
   "outputs": [],
   "source": [
    "Xtrain, Xval, Ytrain, Yval = train_test_split(amazon_corpus.text, amazon_corpus.sentiment, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FLdrvQ3sUl5r"
   },
   "source": [
    "### Tokenization\n",
    "\n",
    "The task of splitting a text into a sequence of symbols (*tokens*) is called *tokenization*. Classically, the tokens correspond to words and punctuation symbols. However, later in the course, we will see alternatives to word-based tokenization.\n",
    "\n",
    "We will not build our own tokenizer, but instead use the tokenizer for English built into the `spacy` library.\n",
    "\n",
    "**Please note**: the first time you use spaCy with some language (English in our case), you need to install a module for that language. See [here](https://spacy.io/usage/models) for a description of how to do this. In short, you typically need to run a command in a shell such as\n",
    "\n",
    "```\n",
    "python -m spacy download en_core_web_sm\n",
    "```\n",
    "Colab users don't need to carry out this step, since spaCy and the English module are already installed by default.\n",
    "\n",
    "When the English module is downloaded, we can load it as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "LOpqHBhiUl5t"
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hZyqkAKPUl5w"
   },
   "source": [
    "Now, we have what we need to do tokenization of English texts.\n",
    "\n",
    "For your convenience, the function below calls the spaCy tokenizer and extracts the token strings. Optionally, we also apply lowercase normalization to the strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "xaCvuj9sUl5w"
   },
   "outputs": [],
   "source": [
    "def tokenize(text, lowercase=True):\n",
    "    if lowercase:\n",
    "        return [t.text.lower() for t in nlp.tokenizer(text)]\n",
    "    else:\n",
    "        return [t.text for t in nlp.tokenizer(text)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ITR4bbWQUl5y"
   },
   "source": [
    "Let's apply the tokenization function to an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "ZhgDcezYUl5z"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[',\n",
       " '12345/689',\n",
       " '-',\n",
       " '123',\n",
       " ']',\n",
       " 'l.',\n",
       " 'ron',\n",
       " 'hubbard',\n",
       " 'went',\n",
       " 'to',\n",
       " 'the',\n",
       " 'u.s',\n",
       " '...',\n",
       " '\"',\n",
       " 'he',\n",
       " 'joined',\n",
       " 'the',\n",
       " 'u.s.',\n",
       " 'army',\n",
       " '!',\n",
       " '!',\n",
       " '!',\n",
       " '\"']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize('[12345/689-123] L. Ron Hubbard went to the U.S... \"He joined the U.S. Army!!!\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "47EKP9WMUl50"
   },
   "source": [
    "### Example: how to find the most frequent words in a dataset\n",
    "\n",
    "When you implement the vocabulary processing below, you will need to compute word frequencies. This can of course be done using standard Python data structures, but the easiest approach is probably to use the specialized dictionary type called [`Counter`](https://docs.python.org/3/library/collections.html#collections.Counter). As the name suggests, this is used in Python when counting things.\n",
    "\n",
    "Here are a few idioms showing how to use the `Counter`. The examples show three different ways to compute the frequencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "XWBLICRRb3Nf"
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "freqs = Counter()\n",
    "for x in Xtrain:\n",
    "    for t in tokenize(x):\n",
    "        freqs[t] += 1\n",
    "\n",
    "#freqs = Counter()\n",
    "#for x in Xtrain:\n",
    "#    freqs.update(tokenize(x))\n",
    "\n",
    "#freqs = Counter(t for x in Xtrain for t in tokenize(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IX5LZVc5Ul52"
   },
   "source": [
    "After building the `Counter`, we have a data structure where each word is mapped to a frequency count.\n",
    "\n",
    "We can then use the method `most_common` to find the items in the dictionary that have the highest frequencies. This method returns a sorted list of item/frequency pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "FA-Kla_bUl52"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the 64533\n",
      ". 62451\n",
      ", 52248\n",
      "and 34242\n",
      "to 31990\n",
      "i 31556\n",
      "a 30595\n",
      "of 26585\n",
      "it 24004\n",
      "is 21982\n"
     ]
    }
   ],
   "source": [
    "for word, freq in freqs.most_common(10):\n",
    "    print(word, freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6dHCj7jefUvz"
   },
   "source": [
    "# Part 1: Preprocessing documents\n",
    "\n",
    "Now, we are ready to implement the utilities we need in order to preprocess documents for machine learning with PyTorch.\n",
    "\n",
    "Your implementation will be done in this class `DocumentPreprocessor`. *Please note* that there are some tests below that check whether you seem to have implemented the methods correctly. If you want, you can work incrementally, so that you make sure that your tests run before moving on to the next step.\n",
    "\n",
    "**Your work:**\n",
    "\n",
    "**1)** Implement the method `build_vocab`.\n",
    "\n",
    "This method takes a training set (inputs `X` and outputs `Y`) and builds two vocabularies, one for the words in the input documents and one for the output labels. These vocabularies are data structures that allow you to map a string to a corresponding integer index.\n",
    "\n",
    "**Requirements:**\n",
    "- The special symbols `PAD` and `UNKNOWN` should correspond to the encoded values 0 and 1, respectively.\n",
    "- The size of the resulting vocabulary should be at most `max_voc_size`, if the user has provided a value of this parameter. If you observe more unique words than `max_voc_size`-2, then you should only include the most frequent words.\n",
    "\n",
    "You can use any data structures you want in this step, but probably you will use some sort of dictionaries. For the `Y` vocabulary, the scikit-learn utility [LabelEncoder](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html) can optionally be used, but a regular dictionary is also OK.\n",
    "\n",
    "**2)** Implement `n_classes` and `voc_size`. This should be trivial after you have solved the previous task.\n",
    "\n",
    "***Testing.*** You can now run the first tests below to validate your implementation, before you proceed to the next task.\n",
    "\n",
    "**3)** Implement `encode`.\n",
    "\n",
    "This method takes a list of input documents `X` and output labels `Y` and returns a list of encoded `x`-`y` pairs, where the `x` part is a list of integers and the `y` part is an integer, using the string-to-integer mappings you created in `build_vocab`. For instance, we could hypothetically have something like\n",
    "\n",
    "```\n",
    "X = ['Great toaster!'], Y = ['pos']  ==>  [([75, 34, 14], 1)]\n",
    "```\n",
    "\n",
    "**Requirements:**\n",
    "- If the user provided a value of the hyperparameter `max_len`, then any document that is longer than this value needs to be truncated.\n",
    "- For words that are not included in the vocabulary, the special symbol `UNKNOWN` (hard-coded to index 1) should be used.\n",
    "\n",
    "**4)** Implement `decode_predictions`.\n",
    "\n",
    "This method simply inverts the symbol-to-integer mapping we use to encode the `Y` values. So we could have something like\n",
    "\n",
    "```\n",
    "[0, 1, 1]  ==>  ['neg', 'pos', 'pos']\n",
    "```\n",
    "The return value should be a list or a NumPy array.\n",
    "\n",
    "***Testing.*** You can now run the tests of `encode` and `decode_predictions` below to validate your implementation, before you proceed to the next task.\n",
    "\n",
    "**5)** Implement `make_batch_tensors`.\n",
    "\n",
    "This function is an example of what is known as a *collator* in PyTorch [`DataLoader`](https://pytorch.org/tutorials/beginner/basics/data_tutorial.html). The `DataLoader` (which you will try below) is a utility that divides the dataset into *batches*. A collator converts a batch into tensors that we can use when training or applying models in PyTorch.\n",
    "\n",
    "It takes a list of encoded instances in the format you created in `encode`. It then returns two PyTorch `Tensor`s, one corresponding to the documents and one to the labels.\n",
    "\n",
    "**Requirements:**\n",
    "- The output tensor corresponding to the `Y` labels should be a one-dimensional tensor (let's call its length `m`).\n",
    "- The output tensor corresponding to the `X` documents should be a two-dimensional tensor of shape `(m, n)` where `n` is the length of the longest document in this batch.\n",
    "- For documents that are shorter than `n`, you need to add the special symbol `PAD` (hard-coded to index 0) at the end so that all documents in the batch are of the same length.\n",
    "\n",
    "***Hint.*** When you pad the documents, do not *modify* the lists you created in `encode`, or you might risk a bug.\n",
    "\n",
    "***Hint.*** You can use `torch.as_tensor` to convert a regular Python list into a tensor.\n",
    "\n",
    "***Testing.*** You can now create a `DataLoader` and run the tests to make sure that you can create tensors for the batches. This batching functionality will be used when you implement the training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yBevnuROfTFK"
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import torch\n",
    "from sklearn.preprocessing import LabelEncoder # optional\n",
    "\n",
    "PAD = \"<PAD>\"\n",
    "UNKNOWN = \"<UNKNOWN>\"\n",
    "\n",
    "class DocumentPreprocessor:\n",
    "    def __init__(self, tokenizer, max_voc_size=None, max_len=None):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_voc_size = max_voc_size\n",
    "        self.max_len = max_len\n",
    "\n",
    "    # (1)\n",
    "    def build_vocab(self, X, Y):\n",
    "        \"\"\"\n",
    "        Build the vocabularies that will be used to encode documents and class labels.\n",
    "\n",
    "        Parameters:\n",
    "          X: a list of document strings.\n",
    "          Y: a list of document class labels.\n",
    "        \"\"\"\n",
    "\n",
    "    # (2)\n",
    "    def n_classes(self):\n",
    "        \"\"\"\n",
    "        Return the number of classes for this classification task.\n",
    "        \"\"\"\n",
    "        return YOUR_CODE_HERE\n",
    "\n",
    "    def voc_size(self):\n",
    "        \"\"\"\n",
    "        Return the number of words in the vocabulary used to encode the document.\n",
    "        \"\"\"\n",
    "        return YOUR_CODE_HERE\n",
    "\n",
    "    # (3)\n",
    "    def encode(self, X, Y):\n",
    "        \"\"\"\n",
    "        Carry out integer encoding of a list of documents X and a corresponding list of labels Y.\n",
    "\n",
    "        Parameters:\n",
    "          X: a list of document strings.\n",
    "          Y: a list of class labels.\n",
    "\n",
    "        Returns:\n",
    "          The list of encoded instances (x, y), where each instance consists of\n",
    "          x: list of integer-encoded tokens in the document\n",
    "          y: integer-encoded class label\n",
    "        \"\"\"\n",
    "        YOUR_CODE_HERE\n",
    "        return []\n",
    "\n",
    "    # (4)\n",
    "    def decode_predictions(self, Y):\n",
    "        \"\"\"\n",
    "        Map a sequence of integer-encoded output labels back to the original labels.\n",
    "\n",
    "        Parameters:\n",
    "          Y: a sequence of integer-encoded class labels.\n",
    "\n",
    "        Returns:\n",
    "          The sequence of class labels in the original format.\n",
    "        \"\"\"\n",
    "        YOUR_CODE_HERE\n",
    "        return []\n",
    "\n",
    "    # (5)\n",
    "    def make_batch_tensors(self, batch):\n",
    "        \"\"\"\n",
    "        Combine a list of instances into two tensors.\n",
    "\n",
    "        Parameters:\n",
    "          batch: a list of instances (x, y), where each instance is an x-y pair as\n",
    "                 described for process_data above.\n",
    "\n",
    "        Returns:\n",
    "          Two PyTorch tensors Xenc, Yenc, where Xenc contains the integer-encoded documents\n",
    "          in this batch, and Yenc the integer-encoded labels.\n",
    "        \"\"\"\n",
    "        YOUR_CODE_HERE\n",
    "\n",
    "        return torch.as_tensor([]), torch.as_tensor([])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NJBKa73vUl57"
   },
   "source": [
    "### Testing your preprocessor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "crqNVGYCUl58"
   },
   "source": [
    "We will now run some tests to check that your implementation seems to work correctly.\n",
    "\n",
    "We first define a preprocessor using the tokenization function we declared above. For testing purposes, we set the max vocabulary size to 256."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fYz9lLHXkO8m"
   },
   "outputs": [],
   "source": [
    "testing_preprocessor = DocumentPreprocessor(tokenizer=tokenize, max_voc_size=256)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "27bm7VB2Ul5_"
   },
   "source": [
    "We use the training set defined above to build the vocabularies. Make sure that the methods `build_vocab`, `n_classes`, and `voc_size` have been implemented at this point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HJtPDTeXUl6A"
   },
   "outputs": [],
   "source": [
    "testing_preprocessor.build_vocab(Xtrain, Ytrain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S91C4ZOJUl6B"
   },
   "source": [
    "**Testing.** The tests below check that the X and Y vocabularies have the right sizes after building the vocabularies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cII7MZNUUl6C"
   },
   "outputs": [],
   "source": [
    "# There are 2 classes in this dataset.\n",
    "assert(testing_preprocessor.n_classes() == 2)\n",
    "# The vocabulary size should be 256 as defined by our parameter.\n",
    "assert(testing_preprocessor.voc_size() == 256)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U8_jRaaqUl6E"
   },
   "source": [
    "### Encoding the documents\n",
    "\n",
    "Now, make sure that the method `encode` (step 3) has been implemented correctly).\n",
    "\n",
    "Then let's take a few example documents and see what happens when we encode them. Make sure you understand why the output looks the way it does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tCacCU8jUl6E"
   },
   "outputs": [],
   "source": [
    "test_docs = ['Great idea.', 'Testing!', 'Another longer document.']\n",
    "test_labels = ['pos', 'neg', 'neg']\n",
    "\n",
    "encoded_docs = testing_preprocessor.encode(test_docs, test_labels)\n",
    "\n",
    "encoded_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vqBtr9H4Ul6F"
   },
   "source": [
    "**Testing.** Now, run the tests below to check that the format of the processed documents seems OK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9pCY9a-zUl6H"
   },
   "outputs": [],
   "source": [
    "# There should be 3 encoded documents.\n",
    "assert(len(encoded_docs) == 3)\n",
    "# There first document has 3 tokens and the second 2 tokens.\n",
    "assert(len(encoded_docs[0][0]) == 3)\n",
    "assert(len(encoded_docs[1][0]) == 2)\n",
    "\n",
    "# The encoded labels should be integers in [0, ..., n_classes-1].\n",
    "assert(encoded_docs[0][1] >= 0)\n",
    "assert(encoded_docs[0][1] < testing_preprocessor.n_classes())\n",
    "\n",
    "# The encoded tokens should be integers in [0, ..., voc_size-1].\n",
    "assert(all(di >= 0 and di < testing_preprocessor.voc_size() for d, _ in encoded_docs for di in d))\n",
    "\n",
    "# The first word in the second document should be out of vocabulary, encoded as 1.\n",
    "assert(encoded_docs[1][0][0] == 1)\n",
    "\n",
    "# If we decode the integer-encoded labels, we should get the original labels back.\n",
    "test_decoded = testing_preprocessor.decode_predictions([i for _, i in encoded_docs])\n",
    "assert(list(test_decoded) == test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ogr7oAN3Ul6H"
   },
   "source": [
    "### Using a DataLoader\n",
    "\n",
    "As already mentioned, PyTorch provides a utility called `DataLoader` that is responsible for creating *batches* from a dataset. When implementing the training loop later, you can then easily iterate through the batches.\n",
    "\n",
    "If you want to understand more about the `DataLoader`, read [this description](https://pytorch.org/docs/stable/data.html) in the PyTorch documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JDFZ2sjcUl6I"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JhQg82TqUl6I"
   },
   "source": [
    "We now create a `DataLoader`. It operates on top of a dataset: in our case, the list of encoded instances. In this example, we set the batch size to 2 and we tell the `DataLoader` to process the instances in order without shuffling.\n",
    "\n",
    "We also need to provide the collator `make_batch_tensors` we defined above. As you know, it takes a batch and creates tensors that we can use with a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YxfNu-UIUl6J"
   },
   "outputs": [],
   "source": [
    "dl = DataLoader(encoded_docs, 2, shuffle=False, collate_fn=testing_preprocessor.make_batch_tensors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ULrAg84qUl6K"
   },
   "source": [
    "This object now acts as any Python iterable. When iterating over this object, we go through all the batches. (If you set `shuffle` to `True`, the order of the instances will be randomized each time you restart the iteration.)\n",
    "\n",
    "Finally, let's run some tests to make sure that your collator is implemented correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YlEoVnRZUl6K"
   },
   "outputs": [],
   "source": [
    "for i, (Xbatch, Ybatch) in enumerate(dl):\n",
    "\n",
    "    # There should be 2 batches since there are 3 instances and we set the batch size to 2.\n",
    "    assert(i < 2)\n",
    "\n",
    "    # The returned values should be tensors.\n",
    "    assert(isinstance(Xbatch, torch.Tensor))\n",
    "    assert(isinstance(Ybatch, torch.Tensor))\n",
    "\n",
    "    if i == 0:\n",
    "        # We set the batch size to 2. The longest document in the first batch has length 3.\n",
    "        assert(Xbatch.shape == (2, 3))\n",
    "        assert(Ybatch.shape == (2,))\n",
    "\n",
    "        # The first token in the second document is out of vocabulary (1).\n",
    "        assert(Xbatch[1, 0] == 1)\n",
    "\n",
    "        # The last token in the second document is padding (0).\n",
    "        assert(Xbatch[1, 2] == 0)\n",
    "    else:\n",
    "        # One document in the last batch. It has length 4.\n",
    "        assert(Xbatch.shape == (1, 4))\n",
    "        assert(Ybatch.shape == (1,))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k8Rw4W1LqtYv"
   },
   "source": [
    "# Part 2: Defining the neural network\n",
    "\n",
    "We will now set up a neural network for text classification. You are free to implement any type of model you want. Typically, we will have the following parts:\n",
    "- A (static) *word representation*: [`nn.Embedding`](https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html)\n",
    "- A *document representation*: some way to aggregate the word embeddings. For instance, you can apply a RNN on the sequence of word embeddings to get a document representation. An even simpler alternative would be that each document is represented as a mean of its word embeddings.\n",
    "- An *output unit*. This should map from the document representation to logits defining the probabilities of the possible classes.\n",
    "\n",
    "**Your work.** Fill in the missing parts of this code, labeled as `YOUR_CODE_HERE`.\n",
    "\n",
    "**Hint.** When implementing `forward`, always keep track of the shapes of tensors after applying each step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "epXFripTq9Jj"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class YourClassifier(nn.Module):\n",
    "  def __init__(self, voc_size, emb_dim):\n",
    "        super().__init__()\n",
    "        YOUR_CODE_HERE\n",
    "\n",
    "  def forward(self, X):\n",
    "        # X is a batch tensor with shape (batch_size, max_doc_length).\n",
    "        # The output should be a tensor with shape (batch_size, number_of_classes)\n",
    "        YOUR_CODE_HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ss7HNr79Ul6L"
   },
   "source": [
    "# Part 3: Implementing the training loop\n",
    "\n",
    "We will now write the code that runs the training loop to train the parameters of a neural network model. This implementation is agnostic with respect to the network structure and we will define the actual model elsewhere.\n",
    "\n",
    "**Your work.** Fill in the missing parts of this code, labeled as `YOUR_CODE_HERE`.\n",
    "\n",
    "In `train_model`, the main part of the training loop, you only need to define the loss function and the optimizer.\n",
    "\n",
    "**Hint.** You may assume that there is an arbitrary number of classes, even though in this example we know that there are 2 classes. So you should use a multiclass loss, not a binary loss. (Our implementation below is also based on the assumption of a multiclass structure.)\n",
    "\n",
    "Most of your work will be done in the function `apply_model`. This function takes a data loader, goes through the batches, and applies the model to each batch. If we are training the model (that is, if an optimizer was provided), we update the model after each batch. Here, you will need to carry out the typical steps in a PyTorch training loop: get the batch tensors from the `DataLoader`, put the tensors on the GPU (if you are using one), apply the model, compute the loss, and update the model. We will also collect some statistics along the way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ARmRzaLAUl6M"
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "def train_model(model, data_train, data_val, par):\n",
    "    \"\"\"Train the model on the given training data.\n",
    "\n",
    "    Parameters:\n",
    "      model:      the PyTorch model that will be trained.\n",
    "      data_train: the DataLoader that generates the input-output batches for training.\n",
    "      data_val:   the DataLoader for validataion.\n",
    "      par:        an object containing all relevant hyperparameters.\n",
    "\n",
    "    Returns:\n",
    "      history:    a dict containing statistics computed over the epochs.\n",
    "    \"\"\"\n",
    "\n",
    "    # Define a loss function that is suitable for a multiclass classification task.\n",
    "    loss_func = YOUR_CODE_HERE\n",
    "\n",
    "    # Define an optimizer that will update the model's parameters.\n",
    "    # You can assume that `par` contains the hyperparameters you need here.\n",
    "    optimizer = YOUR_CODE_HERE\n",
    "\n",
    "    # Contains the statistics that will be returned.\n",
    "    history = defaultdict(list)\n",
    "\n",
    "    progress = tqdm(range(par.n_epochs), 'Epochs')\n",
    "    for epoch in progress:\n",
    "\n",
    "        # Put the model in \"training mode\". Will affect e.g. dropout, batch normalizers.\n",
    "        model.train()\n",
    "\n",
    "        # Run the model on the training set, update the model, and get the training set statistics.\n",
    "        train_loss, train_acc = apply_model(model, data_train, loss_func, optimizer)\n",
    "\n",
    "        # Put the model in \"evaluation mode\". Will affect e.g. dropout, batch normalizers.\n",
    "        model.eval()\n",
    "\n",
    "        # Turn off gradient computation, since we are not updating the model now.\n",
    "        with torch.no_grad():\n",
    "            # Run the model on the validation set and get the training set statistics.\n",
    "            val_loss, val_acc = apply_model(model, data_val, loss_func)\n",
    "\n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['train_acc'].append(train_acc)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['val_acc'].append(val_acc)\n",
    "\n",
    "        progress.set_postfix({'val_loss': f'{val_loss:.2f}', 'val_acc': f'{val_acc:.2f}'})\n",
    "\n",
    "    return history\n",
    "\n",
    "def apply_model(model, data, loss_func, optimizer=None):\n",
    "    \"\"\"Run the neural network for one epoch, using the given batches.\n",
    "    If an optimizer is provided, this is training data and we will update the model\n",
    "    after each batch. Otherwise, this is assumed to be validation data.\n",
    "\n",
    "    Parameters:\n",
    "      model:     the PyTorch model.\n",
    "      data:      the DataLoader that generates the input-output batches.\n",
    "      loss_func: the loss function\n",
    "      optimizer: the optimizer; should be None if we are running on validation data.\n",
    "\n",
    "    Returns the loss and accuracy over the epoch.\"\"\"\n",
    "    n_correct = 0\n",
    "    n_instances = 0\n",
    "    total_loss = 0\n",
    "\n",
    "    device = next(model.parameters()).device\n",
    "\n",
    "    # for each X, Y pair in the batch:\n",
    "    for Xbatch, Ybatch in YOUR_CODE_HERE:\n",
    "\n",
    "        # put X and Y on the device\n",
    "        Xbatch = YOUR_CODE_HERE\n",
    "        Ybatch = YOUR_CODE_HERE\n",
    "\n",
    "        assert(isinstance(Xbatch, torch.Tensor))\n",
    "        assert(isinstance(Ybatch, torch.Tensor))\n",
    "\n",
    "        # forward pass part 1: apply the model on X to get\n",
    "        # the model's outputs for this batch\n",
    "        model_output = YOUR_CODE_HERE\n",
    "\n",
    "        assert(len(model_output.shape) == 2)\n",
    "        assert(model_output.shape[0] == Ybatch.shape[0])\n",
    "\n",
    "        # forward pass part 2: compute the loss by comparing\n",
    "        # the model output to the reference Y values\n",
    "        loss = YOUR_CODE_HERE\n",
    "\n",
    "        assert(not loss.shape)\n",
    "\n",
    "        # update the loss statistics\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # convert the scores computed above into hard decisions\n",
    "        guesses = model_output.argmax(dim=1)\n",
    "\n",
    "        # compute the number of correct predictions and update the statistics\n",
    "        n_correct += (guesses == Ybatch).sum().item()\n",
    "        n_instances += Ybatch.shape[0]\n",
    "\n",
    "        # if we have an optimizer, it means we are processing the training set\n",
    "        # so that the model needs to be updated after each batch\n",
    "        if optimizer:\n",
    "\n",
    "            # reset the gradients\n",
    "            YOUR_CODE_HERE\n",
    "\n",
    "            # backprop to compute the new gradients\n",
    "            YOUR_CODE_HERE\n",
    "\n",
    "            # use the optimizer to update the model\n",
    "            YOUR_CODE_HERE\n",
    "\n",
    "    return total_loss/len(data), n_correct/n_instances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TXW3oyBcUl6V"
   },
   "source": [
    "# Part 4: Training the model\n",
    "\n",
    "Now, combine all the pieces we have created above.\n",
    "\n",
    "- Preprocess the training and validation sets and create corresponding data loaders.\n",
    "- Create a model.\n",
    "- Run the training loop.\n",
    "\n",
    "If your code works, you should see a progress bar advancing after each epoch. The progress bar displays the loss and accuracy scores computed on the validation set after each epoch.\n",
    "\n",
    "You may get different results depending on your implementation as well as random factors due to initialization. A reasonable implementation will typically see accuracies in the range 0.80-0.85 after training for some epochs. If the accuracies are lower or higher than that, you probably have a bug somewhere.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MMAXvMOUUl6V"
   },
   "source": [
    "# Part 5: Inspecting word embeddings\n",
    "\n",
    "The function below computes a set of *nearest neighbors* of a word vector in the word embedding space.\n",
    "\n",
    "**Your work.** Inspect nearest neighbor lists for some selected words. For instance, the following words could be relevant to consider:\n",
    "\n",
    "`fantastic, boring, lens, author`\n",
    "\n",
    "Then retrain the model from scratch, this time learning to classify the *product category* instead of the positive/negative sentiment. (You will repeat the preprocessing steps you carried out in the beginning.) Inspect the nearest neighbor lists of the selected words once again.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3mPVAuPpwK8E"
   },
   "outputs": [],
   "source": [
    "def nearest_neighbors(emb, voc, word, n_neighbors=5):\n",
    "\n",
    "    # Encode the words as integers, and put them into a PyTorch tensor.\n",
    "    word_index = torch.as_tensor([voc.stoi[word]])\n",
    "    word_index = word_index.to(emb.weight.device)\n",
    "\n",
    "    # Look up the embedding for the test word.\n",
    "    test_emb = emb(word_index)\n",
    "\n",
    "    # We'll use a cosine similarity function to find the most similar words.\n",
    "    sim_func = nn.CosineSimilarity(dim=1)\n",
    "    cosine_scores = sim_func(test_emb, emb.weight)\n",
    "\n",
    "    # Find the positions of the highest cosine values.\n",
    "    near_nbr = cosine_scores.topk(n_neighbors+1)\n",
    "    topk_cos = near_nbr.values[1:]\n",
    "    topk_indices = near_nbr.indices[1:]\n",
    "\n",
    "    # Finally, map word indices back to strings, and put the result in a list.\n",
    "    out = [ (voc.itos[ix.item()], cos.item()) for ix, cos in zip(topk_indices, topk_cos) ]\n",
    "    return out"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
