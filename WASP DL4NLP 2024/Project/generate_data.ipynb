{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "from typing import List, Optional\n",
    "\n",
    "def compute_diff(\n",
    "    buggy_code: str, fixed_code: str, context_len: Optional[int] = None\n",
    ") -> List[str]:\n",
    "    \"\"\"\n",
    "    Computes the diff between the buggy and fixed code.\n",
    "    \"\"\"\n",
    "    context_len = (\n",
    "        context_len\n",
    "        if context_len is not None\n",
    "        else max(len(buggy_code), len(fixed_code))\n",
    "    )\n",
    "    with open(\"/tmp/buggy.java\",\"w\") as f: f.write(buggy_code+\"\\n\")\n",
    "    with open(\"/tmp/fixed_code.java\",\"w\") as f: f.write(fixed_code+\"\\n\")\n",
    "    # we want to ignore whitespace changes with -w which does not exist in difflib.unified_diff\n",
    "    # with git diff, we even the name of the changed function in the diff, which helps a lot\n",
    "    cmd = [\"git\",\"diff\",\"--patience\",f\"-U{context_len}\", \"-w\",\"/tmp/buggy.java\",\"/tmp/fixed_code.java\"]\n",
    "    return subprocess.run(cmd, capture_output=True).stdout.decode(\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "megadiff_sf = load_dataset(\"ASSERT-KTH/megadiff-single-function\")\n",
    "megadiff_sf = megadiff_sf.map(lambda x: {\"short_diff\": compute_diff(x[\"buggy_function\"], x[\"fixed_function\"], context_len=3)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def user_prompt(fixed_function: str) -> str:\n",
    "    return f\"\"\"\n",
    "### Fixed Function\n",
    "```java\n",
    "{fixed_function}\n",
    "```\n",
    "\"\"\"\n",
    "\n",
    "def system_prompt() -> str:\n",
    "    return \"\"\"You are an assistant designed to help generate synthetic data for fine-tuning a language model for automatic program repair. You will receive a correctly implemented function as input. Your task is to generate:\n",
    "\n",
    "1. A buggy version of the provided function.\n",
    "2. A unit test method that exposes the behavioral difference between the buggy and fixed versions.\n",
    "3. The stack trace or error message resulting from executing that unit test method with the buggy function.\n",
    "\n",
    "Please ensure the following when generating the output:\n",
    "\n",
    "- The buggy function should have a realistic and plausible error that one might encounter in actual programming.\n",
    "- The unit test MUST be a method and directly target the introduced bug. ONLY the unit test method should be provided, not the entire test suite.\n",
    "- The stack trace or error message must be realistic and show the exact execution of the unit test.\n",
    "- Do NOT add any comment in the code about the bug, the bug injection or any other information.\n",
    "\n",
    "Format the output in the following structure:\n",
    "### Buggy Function\n",
    "```java\n",
    "<Buggy function code>\n",
    "```\n",
    "\n",
    "### Unit Test\n",
    "```java\n",
    "<Unit test code>\n",
    "```\n",
    "\n",
    "### Error Message or Stack Trace\n",
    "```\n",
    "<Error message or stack trace>\n",
    "```\n",
    "\"\"\"\n",
    "\n",
    "def user_prompt_v2(diff: str) -> str:\n",
    "    return f\"\"\"\n",
    "### Diff between fixed and buggy functions\n",
    "```diff\n",
    "{diff}\n",
    "```\n",
    "\"\"\"\n",
    "\n",
    "def system_prompt_v2() -> str:\n",
    "    return \"\"\"You are an assistant designed to help generate synthetic data for fine-tuning a language model for automatic program repair. You will receive a bug-fixing diff as input. Your task is to generate:\n",
    "\n",
    "1. A unit test method that exposes the behavioral difference between the buggy and fixed versions.\n",
    "2. The stack trace or error message resulting from executing the unit test method.\n",
    "\n",
    "Please ensure the following when generating the output:\n",
    "\n",
    "- The unit test MUST be a method and directly target the introduced bug. ONLY the unit test method should be provided, not the entire test suite.\n",
    "- The stack trace or error message must be realistic and show the exact execution of the unit test.\n",
    "- Do NOT add any comment in the code about the bug, the bug injection or any other information.\n",
    "\n",
    "Format the output in the following structure:\n",
    "### Unit Test\n",
    "```java\n",
    "<Unit test code>\n",
    "```\n",
    "\n",
    "### Error Message or Stack Trace\n",
    "```\n",
    "<Error message or stack trace>\n",
    "```\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "\n",
    "load_dotenv()\n",
    "client = OpenAI()\n",
    "\n",
    "def call_openai(system_prompt: str, user_prompt: str, model: str = \"gpt-4o-mini\") -> str:\n",
    "    enc = tiktoken.encoding_for_model(model)\n",
    "    n_tokens = len(enc.encode(system_prompt)) + len(enc.encode(user_prompt))\n",
    "    if n_tokens >= 128000:\n",
    "        return None\n",
    "\n",
    "    completion = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return completion\n",
    "\n",
    "def extract_output(completion):\n",
    "    \"\"\"\n",
    "    Extracts the output from the completion object.\n",
    "    \n",
    "    Returns:\n",
    "    - The generated buggy function\n",
    "    - The generated test case\n",
    "    - The generated test error or stack trace\n",
    "    \"\"\"\n",
    "    message = completion.choices[0].message.content\n",
    "\n",
    "    try:\n",
    "        buggy_function = message.split(\"### Buggy Function\")[1].split(\"```java\")[1].split(\"```\")[0].strip()\n",
    "        test_case = message.split(\"### Unit Test\")[1].split(\"```java\")[1].split(\"```\")[0].strip()\n",
    "        error_message = message.split(\"### Error Message or Stack Trace\")[1].split(\"```\")[1].strip()\n",
    "        return buggy_function, test_case, error_message\n",
    "    except Exception as e:\n",
    "        return None, None, None\n",
    "\n",
    "def extract_output_v2(completion):\n",
    "    \"\"\"\n",
    "    Extracts the output from the completion object.\n",
    "    \n",
    "    Returns:\n",
    "    - The generated test case\n",
    "    - The generated test error or stack trace\n",
    "    \"\"\"\n",
    "    message = completion.choices[0].message.content\n",
    "\n",
    "    try:\n",
    "        test_case = message.split(\"### Unit Test\")[1].split(\"```java\")[1].split(\"```\")[0].strip()\n",
    "        error_message = message.split(\"### Error Message or Stack Trace\")[1].split(\"```\")[1].strip()\n",
    "        return test_case, error_message\n",
    "    except Exception as e:\n",
    "        return None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Bug\n",
      "diff --git a/tmp/buggy.java b/tmp/fixed_code.java\n",
      "index 0fd41ef..e61758b 100644\n",
      "--- a/tmp/buggy.java\n",
      "+++ b/tmp/fixed_code.java\n",
      "@@ -1,24 +1,24 @@\n",
      " private void ping() throws IOException, InterruptedException {\n",
      "         Future<?> f = channel.callAsync(new Ping());\n",
      "         long start = System.currentTimeMillis();\n",
      "         long end = start +timeout;\n",
      " \n",
      "         long remaining;\n",
      "         do {\n",
      "             remaining = end-System.currentTimeMillis();\n",
      "             try {\n",
      "-                f.get(Math.max(0,remaining),MILLISECONDS);\n",
      "+                f.get(remaining, MILLISECONDS);\n",
      "                 return;\n",
      "             } catch (ExecutionException e) {\n",
      "                 if (e.getCause() instanceof RequestAbortedException)\n",
      "                     return; // connection has shut down orderly.\n",
      "                 onDead(e);\n",
      "                 return;\n",
      "             } catch (TimeoutException e) {\n",
      "                 // get method waits \"at most the amount specified in the timeout\",\n",
      "                 // so let's make sure that it really waited enough\n",
      "             }\n",
      "         } while(remaining>0);\n",
      " \n",
      "         onDead(new TimeoutException(\"Ping started on \"+start+\" hasn't completed at \"+System.currentTimeMillis()));//.initCause(e)\n",
      "     }\n",
      "\n",
      "Generated Test Case\n",
      "@Test\n",
      "public void testPingTimeoutException() {\n",
      "    TimeoutException exception = assertThrows(TimeoutException.class, () -> {\n",
      "        BuggyClass buggyInstance = new BuggyClass();\n",
      "        buggyInstance.ping();\n",
      "    });\n",
      "    assertTrue(exception.getMessage().contains(\"Ping started on\"));\n",
      "}\n",
      "Generated Error Message\n",
      "java.util.concurrent.TimeoutException: Ping started on 1632628801000 hasn't completed at 1632628801005\n",
      "\tat BuggyClass.ping(BuggyClass.java:XX)\n",
      "\tat BuggyClassTest.testPingTimeoutException(BuggyClassTest.java:YY)\n"
     ]
    }
   ],
   "source": [
    "completion = call_openai(system_prompt(), user_prompt(megadiff_sf[\"train\"][0][\"fixed_function\"]), model=\"gpt-4o-mini\")\n",
    "\n",
    "\n",
    "buggy_function, test_case, error_message = extract_output(completion)\n",
    "\n",
    "diff = compute_diff(megadiff_sf[\"train\"][0][\"fixed_function\"].strip(), buggy_function.strip())\n",
    "\n",
    "print(\"Generated Bug\")\n",
    "print(diff)\n",
    "\n",
    "print(\"Generated Test Case\")\n",
    "print(test_case)\n",
    "\n",
    "print(\"Generated Error Message\")\n",
    "print(error_message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "diff --git a/tmp/buggy.java b/tmp/fixed_code.java\n",
      "index 675f029..0289ad2 100644\n",
      "--- a/tmp/buggy.java\n",
      "+++ b/tmp/fixed_code.java\n",
      "@@ -7,13 +7,13 @@\n",
      "         boolean isBetter = false;\n",
      "         switch (policy) {\n",
      "             case MINIMIZE:\n",
      "-                if (bestVal > val) {\n",
      "+                if (bestVal > val || nbSol==1) {\n",
      "                     bestVal = val;\n",
      "                     isBetter = true;\n",
      "                 }\n",
      "                 break;\n",
      "             case MAXIMIZE:\n",
      "-                if (bestVal < val) {\n",
      "+                if (bestVal < val || nbSol==1) {\n",
      "                     bestVal = val;\n",
      "                     isBetter = true;\n",
      "                 }\n",
      "\n",
      "@Test\n",
      "public void testPolicyWithSingleSolution() {\n",
      "    double bestVal = 10.0;\n",
      "    double val = 5.0;\n",
      "    int nbSol = 1; // Only one solution\n",
      "    String policy = \"MINIMIZE\"; // Testing MINIMIZE policy\n",
      "\n",
      "    // Call the buggy version of the method\n",
      "    boolean result = buggyPolicyFunction(bestVal, val, nbSol, policy);\n",
      "\n",
      "    // Assert that the bestVal has been updated as expected\n",
      "    assertTrue(result);\n",
      "    assertEquals(5.0, bestVal, 0.001);\n",
      "}\n",
      "java.lang.AssertionError: expected [5.0] but found [10.0]\n",
      "\tat org.junit.Assert.assertEquals(Assert.java:115)\n",
      "\tat org.junit.Assert.assertEquals(Assert.java:144)\n",
      "\tat MyTestClass.testPolicyWithSingleSolution(MyTestClass.java:10)\n"
     ]
    }
   ],
   "source": [
    "completion = call_openai(system_prompt_v2(), user_prompt_v2(megadiff_sf[\"train\"][1][\"short_diff\"]), model=\"gpt-4o-mini\")\n",
    "test_case, error_message = extract_output_v2(completion)\n",
    "\n",
    "print(compute_diff(megadiff_sf[\"train\"][1][\"buggy_function\"], megadiff_sf[\"train\"][1][\"fixed_function\"], context_len=3))\n",
    "\n",
    "print(test_case)\n",
    "\n",
    "print(error_message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/lib/python3.12/typing.py:392: RuntimeWarning: coroutine 'main' was never awaited\n",
      "  def _eval_type(t, globalns, localns, recursive_guard=frozenset()):\n",
      "RuntimeWarning: Enable tracemalloc to get the object allocation traceback\n"
     ]
    }
   ],
   "source": [
    "import concurrent.futures\n",
    "\n",
    "completions = []\n",
    "tasks = []\n",
    "\n",
    "with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "    for i in range(len(megadiff_sf[\"train\"][:1000][\"short_diff\"])):\n",
    "        task = executor.submit(call_openai, system_prompt_v2(), user_prompt_v2(megadiff_sf[\"train\"][i][\"short_diff\"]), model=\"gpt-4o-mini\")\n",
    "        tasks.append(task)\n",
    "\n",
    "    completions = [future.result() for future in tasks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70475a4c7b9f4e9cb6ab9dcded226ac8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "outputs = [extract_output_v2(completion) for completion in completions]\n",
    "\n",
    "megadiff_sf_plus = megadiff_sf[\"train\"].select(range(1000)).add_column(\"generated_test_case\", [output[0] for output in outputs]).add_column(\"generated_error_message\", [output[1] for output in outputs]).add_column(\"completion\", [completion for completion in completions])\n",
    "megadiff_sf_plus.save_to_disk(\"megadiff_sf_plus\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "\n",
    "megadiff_sf_plus = load_from_disk(\"megadiff_sf_plus\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
