{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "599c2033-fb0e-43a3-b419-abd63dfde610",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "64591130-3237-4482-b552-35984b03e342",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['diff', 'is_single_chunk', 'is_single_function', 'buggy_function', 'fixed_function', 'short_diff', 'completion', 'generated_test_case', 'generated_error_message', 'prompt', 'answer'],\n",
       "        num_rows: 63100\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['diff', 'is_single_chunk', 'is_single_function', 'buggy_function', 'fixed_function', 'short_diff', 'completion', 'generated_test_case', 'generated_error_message', 'prompt', 'answer'],\n",
       "        num_rows: 1288\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"ASSERT-KTH/megadiff-sf-synthetic_test_error\", split=\"train\")\n",
    "dataset = dataset.train_test_split(test_size=0.02)\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1149ccc-c186-4557-82b3-59fa66ce482e",
   "metadata": {},
   "source": [
    "# Tokeniser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "84221c24-f59a-4327-8f13-2f4fe39576cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/CodeLlama-7b-Instruct-hf\")\n",
    "tokenizer.add_special_tokens({\"pad_token\":\"<pad>\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5d68ba63-00c2-4fd0-96c0-a833bafd034d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_texts(examples, begin_inst=\"[INST]\", end_inst=\"[\\\\INST]\"):\n",
    "    output_texts = []\n",
    "    for i in range(len(examples['prompt'])):\n",
    "        text = f\"<s>{begin_inst} {examples['prompt'][i]} {end_inst} {examples['answer'][i]}</s>\"\n",
    "        output_texts.append(text)\n",
    "    return output_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fbef43f0-29c4-493b-b8d7-e42c2b97b0e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ceefad37c2e4e4eb9a5966157f526ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50a19d3bddf9486faca91fbfd5959f20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1288 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98b7852cfd0c41d0aa983eae60ffd40a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b83f527657cc477ca23bdb73ea067476",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/1288 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dataset size: 258\n",
      "Validation dataset size: 330\n"
     ]
    }
   ],
   "source": [
    "def tokenize_function(batch):\n",
    "    return tokenizer(format_texts(batch), padding=False, truncation=False)\n",
    "\n",
    "max_seq_length = 1024\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize_function, batched=True)\n",
    "tokenized_dataset = tokenized_dataset.filter(lambda x: len(x[\"input_ids\"]) < max_seq_length, batched=False)\n",
    "\n",
    "print(f\"Training dataset size: {len(tokenized_dataset['train'])}\")\n",
    "print(f\"Validation dataset size: {len(tokenized_dataset['test'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df248a3c-c953-44d5-9dc2-84f49a9b1ccd",
   "metadata": {},
   "source": [
    "# Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4ba66936-a465-44b9-bca5-04ba4f2800c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72306de33f3242cba53fea66f525662e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "from peft import LoraConfig\n",
    "from trl import SFTConfig, SFTTrainer\n",
    "from accelerate import PartialState\n",
    "\n",
    "import torch\n",
    "\n",
    "# FIXME: to enable more than 1 sample per batch, the extra padding token must be set in the model and the embedding layer resized\n",
    "# the current workaround simply adds a new pad_token so that the eos_token is not ignored during training, since the models needs to learn when to stop\n",
    "device_string = PartialState().process_index\n",
    "model = AutoModelForCausalLM.from_pretrained(\"meta-llama/CodeLlama-7b-Instruct-hf\",\n",
    "                                             torch_dtype=torch.bfloat16,\n",
    "                                             device_map={'':device_string})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e63ef64d-1bbe-4fdb-8ebc-5ba78afde8e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import DataCollatorForCompletionOnlyLM\n",
    "\n",
    "response_template_with_context = \"[\\\\INST]\"\n",
    "response_template_ids = tokenizer.encode(response_template_with_context, add_special_tokens=False)\n",
    "\n",
    "collator = DataCollatorForCompletionOnlyLM(response_template_ids, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "df90634c-d490-4a79-899d-86a6ddecfd90",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/site-packages/trl/trainer/sft_trainer.py:408: UserWarning: You passed a tokenizer with `padding_side` not equal to `right` to the SFTTrainer. This might lead to some unexpected behaviour due to overflow issues when training a model in half-precision. You might consider adding `tokenizer.padding_side = 'right'` to your code.\n",
      "  warnings.warn(\n",
      "Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "max_steps is given, it will override any value given in num_train_epochs\n"
     ]
    }
   ],
   "source": [
    "peft_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "sft_config = SFTConfig(\n",
    "    output_dir='tmp_trainer',\n",
    "    learning_rate=5e-4,\n",
    "    num_train_epochs=1,\n",
    "    max_seq_length=max_seq_length,\n",
    "    per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_size=1,\n",
    "    packing=False,\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"test\"],\n",
    "    args=sft_config,\n",
    "    peft_config=peft_config,\n",
    "    data_collator=collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2c692f3e-c8d3-4797-b66c-4c56de347c50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[    1,     1,   518, 25580, 29962,   887,   526,   385, 18428,  1824,\n",
      "         26032,  5780, 29889,  3575,  3414,   338,   304,  2329,   278,  4944,\n",
      "          6494,  1927,   775, 29889,    13,    13,  1576,  1494,   775,  3743,\n",
      "           263,  6494,  1927,   740, 29901,    13, 28956,  1645,    13,  1678,\n",
      "           970,  1780,  1243,  8566,  2631,  2855,  6185,  2631,  4873,  8614,\n",
      "           580,   426,    13,   308,    13,  4706,  8427,  8614,  2295,   353,\n",
      "          8427,  8614, 29889,  4381,  8614,   580,    13, 18884,   869,   351,\n",
      "           929, 29931,   293,  1947, 29898, 21076,  1674,   261, 29889, 29933,\n",
      "          5371, 29897,    13, 18884,   869, 12007, 29898, 21076,  1674,   261,\n",
      "         29889, 29933,  5371, 29897,    13, 18884,   869,  3827, 29898, 21076,\n",
      "          1674,   261, 29889,  1806, 29963, 29897,    13, 18884,   869,  9961,\n",
      "           345, 29898, 21076,  1674,   261, 29889,  1806, 29963, 29897,    13,\n",
      "         18884,   869, 12007, 29898, 21076,  1674,   261, 29889,  1806, 29963,\n",
      "         29897,    13, 18884,   869,  3827, 29898, 21076,  1674,   261, 29889,\n",
      "          6006, 29943,  5265, 29990, 29897,    13, 18884,   869,  8552,  3047,\n",
      "          6572,  1133,   663, 29898,  1888, 23975,  1293, 29889,   974, 29898,\n",
      "         21076,  1674,   261, 29889,  1806, 29963, 29892, 12904,   261, 29889,\n",
      "         29933,  5371,   876,    13, 18884,   869,  8552,  3047, 29956,   768,\n",
      "           519, 29903,  2863, 29898,  1888, 23975,  2697, 29889,   974, 29898,\n",
      "         21076,  1674,   261, 29889,  1806, 29963,  2483,    13,   308,    13,\n",
      "          4706,  6535,  2061, 26895,   353,   775, 29883, 29889,   517,  4051,\n",
      "          2061, 29898,  2917,   416,    13,   308,    13,  4706,  8427,  8614,\n",
      "          1602,  6797,   353,   775, 29883, 29889,  3166,  4051,  2061, 29898,\n",
      "         21627,   416,    13,   308,    13,  4706,  4974,  5574, 29898,  7099,\n",
      "          6797, 29889,   275, 10861, 29898, 21076,  1674,   261, 29889,  1806,\n",
      "         29963,  2483,    13,  4706,  4974,  5574, 29898,  7099,  6797, 29889,\n",
      "           275, 10861, 29898, 21076,  1674,   261, 29889, 29933,  5371,  2483,\n",
      "            13,  4706,  4974,  7058, 29898,  7099,  6797, 29889,   536,  2018,\n",
      "         21076,  1674,   414,  2141,   657, 29898, 29900,   511,   338, 29898,\n",
      "         21076,  1674,   261, 29889,  1806, 29963,  2483,    13,  4706,  4974,\n",
      "          7058, 29898,  7099,  6797, 29889,   536,  2018, 21076,  1674,   414,\n",
      "          2141,   657, 29898, 29896,   511,   338, 29898, 21076,  1674,   261,\n",
      "         29889, 29933,  5371,  2483,    13,  4706,  4974,  5574, 29898,  7099,\n",
      "          6797, 29889,  3068,  6113, 29898, 21076,  1674,   261, 29889,  1806,\n",
      "         29963,  2483,    13,  4706,  4974, 14776, 29898,  4435,  5709, 29889,\n",
      "         16244,  3352, 29892,  1602,  6797, 29889,  4882,  2776, 29898, 21076,\n",
      "          1674,   261, 29889,  6006, 29943,  5265, 29990,  2483,    13,  4706,\n",
      "          4974, 14776, 29898,  4435,  5709, 29889,  1430,  6181,  6181, 29892,\n",
      "          1602,  6797, 29889,  4882,  2776, 29898, 21076,  1674,   261, 29889,\n",
      "          7228,  2483,    13,  1678,   500,    13,    13, 28956,    13,    13,\n",
      "          1576,   775,  8465,   278,  1494,  1243, 29901,    13, 28956,  1645,\n",
      "            13, 29992,  3057,    13,  3597,  1780,  1243,  5709,  2776, 21076,\n",
      "          1674,   261,  7228,   580,   426,    13,  1678,  3826,  6119,  1602,\n",
      "          6797,   353,   716,  3826,  6119,   890,    13,  1678,   849, 17090,\n",
      "           278,  2847,  6230,   310,   278,  3826,  6119,   322,   967,  2106,\n",
      "            13,  1678,  1602,  6797, 29889, 24926,   890,   849, 22108,   411,\n",
      "          3935,  6230,   775,   565,  4312,    13,  1678,  4974, 14776, 29898,\n",
      "          4435,  5709, 29889,  1430,  6181,  6181, 29892,  1602,  6797, 29889,\n",
      "          4882,  2776, 29898, 21076,  1674,   261, 29889,  7228,  2483,    13,\n",
      "         29913,    13, 28956,    13,    13,  3047,   278,  1494,  1243,  1059,\n",
      "         29901,    13, 28956,    13,  1645, 29889,  3893, 29889, 14697,   291,\n",
      "          2392, 29901, 29871,    13,  1252,  6021,   584,  1430,  6181,  6181,\n",
      "            13,  2865,   950,   259,   584, 26612,  6227,  6181, 29918, 23711,\n",
      "          6181, 29928,    13,    12,   271,  1638, 29889, 18491, 29889, 14697,\n",
      "         29889,  9294, 14776, 29898, 14697, 29889,  1645, 29901, 29896, 29896,\n",
      "         29945, 29897,    13,    12,   271,  1638, 29889, 18491, 29889, 14697,\n",
      "         29889,  9294, 14776, 29898, 14697, 29889,  1645, 29901, 29896, 29946,\n",
      "         29946, 29897,    13,    12,   271,  1619,  3057,  2385, 29889,  1688,\n",
      "          5709,  2776, 21076,  1674,   261,  7228, 29898,  3421,  3057,  2385,\n",
      "         29889,  1645, 29901, 29896, 29900, 29897,    13, 28956,    13,    13,\n",
      "         12148,  3867,   263,  4343,  1873,   310,   278,  6494,  1927,   740,\n",
      "         29892,   322,   871,   393,   740, 29901,    13, 12452, 25580, 29962,\n",
      "          7521,  1645,    13,  1678,   970,  1780,  1243,  8566,  2631,  2855,\n",
      "          6185,  2631,  4873,  8614,   580,   426,    13,   308,    13,  4706,\n",
      "          8427,  8614,  2295,   353,  8427,  8614, 29889,  4381,  8614,   580,\n",
      "            13, 18884,   869,   351,   929, 29931,   293,  1947, 29898, 21076,\n",
      "          1674,   261, 29889, 29933,  5371, 29897,    13, 18884,   869, 12007,\n",
      "         29898, 21076,  1674,   261, 29889, 29933,  5371, 29897,    13, 18884,\n",
      "           869,  3827, 29898, 21076,  1674,   261, 29889,  1806, 29963, 29897,\n",
      "            13, 18884,   869,  9961,   345, 29898, 21076,  1674,   261, 29889,\n",
      "          1806, 29963, 29897,    13, 18884,   869, 12007, 29898, 21076,  1674,\n",
      "           261, 29889,  1806, 29963, 29897,    13, 18884,   869,  3827, 29898,\n",
      "         21076,  1674,   261, 29889,  6006, 29943,  5265, 29990, 29897,    13,\n",
      "         18884,   869,  8552,  3047,  6572,  1133,   663, 29898,  1888, 23975,\n",
      "          1293, 29889,   974, 29898, 21076,  1674,   261, 29889,  1806, 29963,\n",
      "         29892, 12904,   261, 29889, 29933,  5371,   876,    13, 18884,   869,\n",
      "          8552,  3047, 29956,   768,   519, 29903,  2863, 29898,  1888, 23975,\n",
      "          2697, 29889,   974, 29898, 21076,  1674,   261, 29889,  1806, 29963,\n",
      "          2483,    13,   308,    13,  4706,  6535,  2061, 26895,   353,   775,\n",
      "         29883, 29889,   517,  4051,  2061, 29898,  2917,   416,    13,   308,\n",
      "            13,  4706,  8427,  8614,  1602,  6797,   353,   775, 29883, 29889,\n",
      "          3166,  4051,  2061, 29898, 21627,   416,    13,   308,    13,  4706,\n",
      "          4974,  5574, 29898,  7099,  6797, 29889,   275, 10861, 29898, 21076,\n",
      "          1674,   261, 29889,  1806, 29963,  2483,    13,  4706,  4974,  5574,\n",
      "         29898,  7099,  6797, 29889,   275, 10861, 29898, 21076,  1674,   261,\n",
      "         29889, 29933,  5371,  2483,    13,  4706,  4974,  7058, 29898,  7099,\n",
      "          6797, 29889,   536,  2018, 21076,  1674,   414,  2141,   657, 29898,\n",
      "         29900,   511,   338, 29898, 21076,  1674,   261, 29889,  1806, 29963,\n",
      "          2483,    13,  4706,  4974,  7058, 29898,  7099,  6797, 29889,   536,\n",
      "          2018, 21076,  1674,   414,  2141,   657, 29898, 29896,   511,   338,\n",
      "         29898, 21076,  1674,   261, 29889, 29933,  5371,  2483,    13,  4706,\n",
      "          4974,  5574, 29898,  7099,  6797, 29889,  3068,  6113, 29898, 21076,\n",
      "          1674,   261, 29889,  1806, 29963,  2483,    13,  4706,  4974, 14776,\n",
      "         29898,  4435,  5709, 29889, 16244,  3352, 29892,  1602,  6797, 29889,\n",
      "          4882,  2776, 29898, 21076,  1674,   261, 29889,  6006, 29943,  5265,\n",
      "         29990,  2483,    13,  4706,  4974, 14776, 29898,  4435,  5709, 29889,\n",
      "         26612,  6227,  6181, 29918, 23711,  6181, 29928, 29892,  1602,  6797,\n",
      "         29889,  4882,  2776, 29898, 21076,  1674,   261, 29889,  7228,  2483,\n",
      "            13,  1678,   500,    13,    13, 28956,     2]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1]], device='cuda:0'), 'labels': tensor([[ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          7521,  1645,    13,  1678,   970,  1780,  1243,  8566,  2631,  2855,\n",
      "          6185,  2631,  4873,  8614,   580,   426,    13,   308,    13,  4706,\n",
      "          8427,  8614,  2295,   353,  8427,  8614, 29889,  4381,  8614,   580,\n",
      "            13, 18884,   869,   351,   929, 29931,   293,  1947, 29898, 21076,\n",
      "          1674,   261, 29889, 29933,  5371, 29897,    13, 18884,   869, 12007,\n",
      "         29898, 21076,  1674,   261, 29889, 29933,  5371, 29897,    13, 18884,\n",
      "           869,  3827, 29898, 21076,  1674,   261, 29889,  1806, 29963, 29897,\n",
      "            13, 18884,   869,  9961,   345, 29898, 21076,  1674,   261, 29889,\n",
      "          1806, 29963, 29897,    13, 18884,   869, 12007, 29898, 21076,  1674,\n",
      "           261, 29889,  1806, 29963, 29897,    13, 18884,   869,  3827, 29898,\n",
      "         21076,  1674,   261, 29889,  6006, 29943,  5265, 29990, 29897,    13,\n",
      "         18884,   869,  8552,  3047,  6572,  1133,   663, 29898,  1888, 23975,\n",
      "          1293, 29889,   974, 29898, 21076,  1674,   261, 29889,  1806, 29963,\n",
      "         29892, 12904,   261, 29889, 29933,  5371,   876,    13, 18884,   869,\n",
      "          8552,  3047, 29956,   768,   519, 29903,  2863, 29898,  1888, 23975,\n",
      "          2697, 29889,   974, 29898, 21076,  1674,   261, 29889,  1806, 29963,\n",
      "          2483,    13,   308,    13,  4706,  6535,  2061, 26895,   353,   775,\n",
      "         29883, 29889,   517,  4051,  2061, 29898,  2917,   416,    13,   308,\n",
      "            13,  4706,  8427,  8614,  1602,  6797,   353,   775, 29883, 29889,\n",
      "          3166,  4051,  2061, 29898, 21627,   416,    13,   308,    13,  4706,\n",
      "          4974,  5574, 29898,  7099,  6797, 29889,   275, 10861, 29898, 21076,\n",
      "          1674,   261, 29889,  1806, 29963,  2483,    13,  4706,  4974,  5574,\n",
      "         29898,  7099,  6797, 29889,   275, 10861, 29898, 21076,  1674,   261,\n",
      "         29889, 29933,  5371,  2483,    13,  4706,  4974,  7058, 29898,  7099,\n",
      "          6797, 29889,   536,  2018, 21076,  1674,   414,  2141,   657, 29898,\n",
      "         29900,   511,   338, 29898, 21076,  1674,   261, 29889,  1806, 29963,\n",
      "          2483,    13,  4706,  4974,  7058, 29898,  7099,  6797, 29889,   536,\n",
      "          2018, 21076,  1674,   414,  2141,   657, 29898, 29896,   511,   338,\n",
      "         29898, 21076,  1674,   261, 29889, 29933,  5371,  2483,    13,  4706,\n",
      "          4974,  5574, 29898,  7099,  6797, 29889,  3068,  6113, 29898, 21076,\n",
      "          1674,   261, 29889,  1806, 29963,  2483,    13,  4706,  4974, 14776,\n",
      "         29898,  4435,  5709, 29889, 16244,  3352, 29892,  1602,  6797, 29889,\n",
      "          4882,  2776, 29898, 21076,  1674,   261, 29889,  6006, 29943,  5265,\n",
      "         29990,  2483,    13,  4706,  4974, 14776, 29898,  4435,  5709, 29889,\n",
      "         26612,  6227,  6181, 29918, 23711,  6181, 29928, 29892,  1602,  6797,\n",
      "         29889,  4882,  2776, 29898, 21076,  1674,   261, 29889,  7228,  2483,\n",
      "            13,  1678,   500,    13,    13, 28956,     2]], device='cuda:0')}\n"
     ]
    }
   ],
   "source": [
    "for batch in trainer.get_train_dataloader():\n",
    "    print(batch)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ee4890be-187b-48d6-afb7-8846e0965675",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1/1 00:00, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "('codellama-instruct-repair/tokenizer_config.json',\n",
       " 'codellama-instruct-repair/special_tokens_map.json',\n",
       " 'codellama-instruct-repair/tokenizer.json')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()\n",
    "trainer.save_state()\n",
    "trainer.save_model(output_dir=\"codellama-instruct-repair\")\n",
    "tokenizer.save_pretrained(save_directory=\"codellama-instruct-repair\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "166a33e0-69c4-4016-b601-bd7dac8b9020",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
