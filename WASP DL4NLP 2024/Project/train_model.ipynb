{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "599c2033-fb0e-43a3-b419-abd63dfde610",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "64591130-3237-4482-b552-35984b03e342",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['diff', 'is_single_chunk', 'is_single_function', 'buggy_function', 'fixed_function', 'short_diff', 'completion', 'generated_test_case', 'generated_error_message', 'prompt', 'answer'],\n",
       "        num_rows: 63100\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['diff', 'is_single_chunk', 'is_single_function', 'buggy_function', 'fixed_function', 'short_diff', 'completion', 'generated_test_case', 'generated_error_message', 'prompt', 'answer'],\n",
       "        num_rows: 1288\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"ASSERT-KTH/megadiff-sf-synthetic_test_error\", split=\"train\")\n",
    "dataset = dataset.train_test_split(test_size=0.02)\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1149ccc-c186-4557-82b3-59fa66ce482e",
   "metadata": {},
   "source": [
    "# Tokeniser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "84221c24-f59a-4327-8f13-2f4fe39576cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/CodeLlama-7b-Instruct-hf\")\n",
    "tokenizer.add_special_tokens({\"pad_token\":\"<pad>\"})\n",
    "tokenizer.padding_side = 'right'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5d68ba63-00c2-4fd0-96c0-a833bafd034d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_texts(examples, begin_inst=\"[INST]\", end_inst=\"[\\\\INST]\"):\n",
    "    output_texts = []\n",
    "    for i in range(len(examples['prompt'])):\n",
    "        text = f\"<s>{begin_inst} {examples['prompt'][i]} {end_inst} {examples['answer'][i]}</s>\"\n",
    "        output_texts.append(text)\n",
    "    return output_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fbef43f0-29c4-493b-b8d7-e42c2b97b0e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parameter 'indices'=<generator object <genexpr> at 0x1455ccbacc70> of the transform datasets.arrow_dataset.Dataset.select couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39ea8ec125604531ac24abb6dac7509f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8787ca670ebe4cd59bbb920c652e69c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5fd49ba1a8745c5afa85e78b85733eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47cd9b0bf97c42b8ab60dd006882a2b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dataset size: 255\n",
      "Validation dataset size: 26\n"
     ]
    }
   ],
   "source": [
    "def tokenize_function(batch):\n",
    "    return tokenizer(format_texts(batch), padding=False, truncation=False)\n",
    "\n",
    "max_seq_length = 1024\n",
    "\n",
    "### DEBUG\n",
    "dataset[\"train\"] = dataset[\"train\"].select(i for i in range(1000))\n",
    "dataset[\"test\"] = dataset[\"test\"].select(i for i in range(100))\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize_function, batched=True)\n",
    "tokenized_dataset = tokenized_dataset.filter(lambda x: len(x[\"input_ids\"]) < max_seq_length, batched=False)\n",
    "\n",
    "print(f\"Training dataset size: {len(tokenized_dataset['train'])}\")\n",
    "print(f\"Validation dataset size: {len(tokenized_dataset['test'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df248a3c-c953-44d5-9dc2-84f49a9b1ccd",
   "metadata": {},
   "source": [
    "# Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4ba66936-a465-44b9-bca5-04ba4f2800c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8049e15edfc947449efd895967bd7784",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "from peft import LoraConfig\n",
    "from trl import SFTConfig, SFTTrainer\n",
    "from accelerate import PartialState\n",
    "\n",
    "import torch\n",
    "\n",
    "# FIXME: to enable more than 1 sample per batch, the extra padding token must be set in the model and the embedding layer resized\n",
    "# the current workaround simply adds a new pad_token so that the eos_token is not ignored during training, since the models needs to learn when to stop\n",
    "device_string = PartialState().process_index\n",
    "model = AutoModelForCausalLM.from_pretrained(\"meta-llama/CodeLlama-7b-Instruct-hf\",\n",
    "                                             torch_dtype=torch.bfloat16,\n",
    "                                             device_map={'':device_string})\n",
    "model.config.pad_token_id = tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e63ef64d-1bbe-4fdb-8ebc-5ba78afde8e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import DataCollatorForCompletionOnlyLM\n",
    "\n",
    "response_template_with_context = \"[\\\\INST]\"\n",
    "response_template_ids = tokenizer.encode(response_template_with_context, add_special_tokens=False)\n",
    "\n",
    "collator = DataCollatorForCompletionOnlyLM(response_template_ids, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "df90634c-d490-4a79-899d-86a6ddecfd90",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/site-packages/trl/trainer/sft_trainer.py:408: UserWarning: You passed a tokenizer with `padding_side` not equal to `right` to the SFTTrainer. This might lead to some unexpected behaviour due to overflow issues when training a model in half-precision. You might consider adding `tokenizer.padding_side = 'right'` to your code.\n",
      "  warnings.warn(\n",
      "Detected kernel version 4.18.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "max_steps is given, it will override any value given in num_train_epochs\n"
     ]
    }
   ],
   "source": [
    "peft_config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "sft_config = SFTConfig(\n",
    "    output_dir='tmp_trainer',\n",
    "    learning_rate=5e-4,\n",
    "    num_train_epochs=1,\n",
    "## DEBUG\n",
    "    max_steps=1,\n",
    "    max_seq_length=max_seq_length,\n",
    "    per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_size=1,\n",
    "    packing=False,\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"test\"],\n",
    "    args=sft_config,\n",
    "    peft_config=peft_config,\n",
    "    data_collator=collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2c692f3e-c8d3-4797-b66c-4c56de347c50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[    1,     1,   518, 25580, 29962,   887,   526,   385, 18428,  1824,\n",
      "         26032,  5780, 29889,  3575,  3414,   338,   304,  2329,   278,  4944,\n",
      "          6494,  1927,   775, 29889,    13,    13,  1576,  1494,   775,  3743,\n",
      "           263,  6494,  1927,   740, 29901,    13, 28956,  1645,    13,  4706,\n",
      "           970,  4669,  1246,   580,  8026, 24490, 29892, 19014,  5308,  2451,\n",
      "           426,    13,  9651,  3497,  7090,  2283,   353,  1870, 29936,    13,\n",
      "          9651,   565,   313,  8173,  9170,  2804,  1870, 29897,   426,    13,\n",
      "         18884,   501, 11150,  1820,   353,   679,  2558, 29898,  3177,   416,\n",
      "            13, 18884,  7090,  2283,   353,   716,  3497, 29898,  8173,  9170,\n",
      "         29892,  1820, 29889,  7711,   580,   718, 11393,  4130, 29889, 18828,\n",
      "          1496,    13, 18884,   565,   313,  8173,  2283, 29889,  9933,  3101,\n",
      "           426,    13,   462,  1678,   736,  1303,  1523, 13120,  2061, 29898,\n",
      "          8173,  2283, 29892,  2943, 29889,   657,  4775,  2141,   657, 29903,\n",
      "         27685,  2467,  2141,   657,  2110,  1463,  1542,  3310,    13, 18884,\n",
      "           500,    13,  9651,   500,    13,    13,  9651,   849,  1939,  1203,\n",
      "           515,   278,  7797,  2133,  4840, 29892,  1235, 29915, 29879,  1018,\n",
      "           304,  1207,   697,    13,  9651,  4669,  5446,   353, 13341, 29889,\n",
      "          7302, 29898,  3177,   416,    13,  9651,   565,   313,  5415,  1275,\n",
      "          1870, 29897,   426,    13, 18884,  3183,   716, 19014,  5308,  2451,\n",
      "           890,    13,  9651,   500,    13,    13,  9651,   849,  1286,  4078,\n",
      "           372,   304,  8086, 29892,   565,  1950,   322,  1661, 29899,  4304,\n",
      "            13,  9651,   565,   313,  5415, 28919, 18896, 13902,  2607,  7090,\n",
      "          2283,  2804,  1870, 29897,   426,    13, 18884,  2436,  1523, 13120,\n",
      "          2061, 29898,  8173,  2283, 29892,  5446,   416,    13,  9651,   500,\n",
      "          1683,   565,   313,  5415,  2804,  1870, 29897,   426,    13, 18884,\n",
      "         17927, 29889, 25442,   703,   348, 15550, 13902,  1203,  6571, 13213,\n",
      "           630,   613,  5446,   416,    13,  9651,   500,    13,    13,  9651,\n",
      "           736,  5446, 29936,    13,  4706,   500,    13,    13, 28956,    13,\n",
      "            13,  1576,   775,  8465,   278,  1494,  1243, 29901,    13, 28956,\n",
      "          1645,    13,  5215,  1638, 29889, 18491, 29889,  3057, 29936,    13,\n",
      "          5215,  2294,  1638, 29889, 18491, 29889, 14697, 26355,    13,    13,\n",
      "          3597,   770, 18896, 13902,  3057,   426,    13,    13,  1678,   732,\n",
      "          3057,    13,  1678,   970,  1780,  1243,  2061,  9125,  2133,  3047,\n",
      "          7327, 10408,  2283,   580,   426,    13,  4706,  4669,  5446,   353,\n",
      "           716,  1714,   703,  1688,  1203,  1496,    13,  4706,   849,  2266,\n",
      "           591,  5251,   278,  7090,  2283,   338,  1870,    13,  4706,  1714,\n",
      "          7090,  2283,   353,  1870, 29936,    13,    13,  4706,  7223,  1121,\n",
      "           353,  4078,  2061, 29898,  5415, 29892,  7090,  2283,   416,   849,\n",
      "           910,  1158,  1027,   352,  1078,   278,  1203, 14238,    13,  4706,\n",
      "          4974,  8824, 29898,  2914,   416,   849,  1222,  1103,   292,   278,\n",
      "          4078,  5858,   304,  4418,  4047,  2705,   373,   263,  1870,  7090,\n",
      "          2283,    13,  1678,   500,    13,   268,    13,  1678,  2024,  7223,\n",
      "          4078,  2061, 29898,  2061,  5446, 29892,  1714,  7090,  2283, 29897,\n",
      "           426,    13,  4706,   849,  3185,   950,  7797,  2133,  5900,  5771,\n",
      "          1244, 29892,   541,  1027,   352,  1078,  2437, 10610,   373,   278,\n",
      "          6494,  1927,   775,    13,  4706,   849,  7106,   292,  2089,   304,\n",
      "         12266, 10672,    13,  4706,   736,  2089, 29936,    13,  1678,   500,\n",
      "            13, 29913,    13, 28956,    13,    13,  3047,   278,  1494,  1243,\n",
      "          1059, 29901,    13, 28956,    13,  1645, 29889,  3893, 29889, 14697,\n",
      "           291,  2392, 29901,  3806, 29901, 29966,  4541, 29958,   541,   471,\n",
      "         29901, 29966,  3009, 29958,    13,  1678,   472,  1638, 29889, 18491,\n",
      "         29889, 14697, 29889, 14057, 29898, 14697, 29889,  1645, 29901, 29929,\n",
      "         29906, 29897,    13,  1678,   472,  1638, 29889, 18491, 29889, 14697,\n",
      "         29889,  9294,  8824, 29898, 14697, 29889,  1645, 29901, 29955, 29953,\n",
      "         29897,    13,  1678,   472,  1638, 29889, 18491, 29889, 14697, 29889,\n",
      "          9294,  8824, 29898, 14697, 29889,  1645, 29901, 29947, 29945, 29897,\n",
      "            13,  1678,   472, 18896, 13902,  3057, 29889,  1688,  2061,  9125,\n",
      "          2133,  3047,  7327, 10408,  2283, 29898,  9125, 13902,  3057, 29889,\n",
      "          1645, 29901, 29896, 29900, 29897,    13, 28956,    13,    13, 12148,\n",
      "          3867,   263,  4343,  1873,   310,   278,  6494,  1927,   740, 29892,\n",
      "           322,   871,   393,   740, 29901,    13, 12452, 25580, 29962,  7521,\n",
      "          1645,    13,  4706,   970,  4669,  1246,   580,  8026, 24490, 29892,\n",
      "         19014,  5308,  2451,   426,    13,  9651,  3497,  7090,  2283,   353,\n",
      "          1870, 29936,    13,  9651,   565,   313,  8173,  9170,  2804,  1870,\n",
      "         29897,   426,    13, 18884,   501, 11150,  1820,   353,   679,  2558,\n",
      "         29898,  3177,   416,    13, 18884,  7090,  2283,   353,   716,  3497,\n",
      "         29898,  8173,  9170, 29892,  1820, 29889,  7711,   580,   718, 11393,\n",
      "          4130, 29889, 18828,  1496,    13, 18884,   565,   313,  8173,  2283,\n",
      "         29889,  9933,  3101,   426,    13,   462,  1678,   736,  1303,  1523,\n",
      "         13120,  2061, 29898,  8173,  2283, 29892,  2943, 29889,   657,  4775,\n",
      "          2141,   657, 29903, 27685,  2467,  2141,   657,  2110,  1463,  1542,\n",
      "          3310,    13, 18884,   500,    13,  9651,   500,    13,    13,  9651,\n",
      "           849,  1939,  1203,   515,   278,  7797,  2133,  4840, 29892,  1235,\n",
      "         29915, 29879,  1018,   304,  1207,   697,    13,  9651,  4669,  5446,\n",
      "           353, 13341, 29889,  7302, 29898,  3177,   416,    13,  9651,   565,\n",
      "           313,  5415,  1275,  1870, 29897,   426,    13, 18884,  3183,   716,\n",
      "         19014,  5308,  2451,   890,    13,  9651,   500,    13,    13,  9651,\n",
      "           849,  1286,  4078,   372,   304,  8086, 29892,   565,  1950,   322,\n",
      "          1661, 29899,  4304,    13,  9651,   565,   313,  5415, 28919, 18896,\n",
      "         13902, 29897,   426,    13, 18884,   565,   313,  8173,  2283,  2804,\n",
      "          1870, 29897,   426,    13,   462,  1678,  2436,  1523, 13120,  2061,\n",
      "         29898,  8173,  2283, 29892,  5446,   416,    13, 18884,   500,    13,\n",
      "          9651,   500,  1683,   426,    13, 18884, 17927, 29889, 25442,   703,\n",
      "           348, 15550, 13902,  1203,  6571, 13213,   630,   613,  5446,   416,\n",
      "            13,  9651,   500,    13,    13,  9651,   736,  5446, 29936,    13,\n",
      "          4706,   500,    13,    13, 28956,     2]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], device='cuda:0'), 'labels': tensor([[ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  7521,\n",
      "          1645,    13,  4706,   970,  4669,  1246,   580,  8026, 24490, 29892,\n",
      "         19014,  5308,  2451,   426,    13,  9651,  3497,  7090,  2283,   353,\n",
      "          1870, 29936,    13,  9651,   565,   313,  8173,  9170,  2804,  1870,\n",
      "         29897,   426,    13, 18884,   501, 11150,  1820,   353,   679,  2558,\n",
      "         29898,  3177,   416,    13, 18884,  7090,  2283,   353,   716,  3497,\n",
      "         29898,  8173,  9170, 29892,  1820, 29889,  7711,   580,   718, 11393,\n",
      "          4130, 29889, 18828,  1496,    13, 18884,   565,   313,  8173,  2283,\n",
      "         29889,  9933,  3101,   426,    13,   462,  1678,   736,  1303,  1523,\n",
      "         13120,  2061, 29898,  8173,  2283, 29892,  2943, 29889,   657,  4775,\n",
      "          2141,   657, 29903, 27685,  2467,  2141,   657,  2110,  1463,  1542,\n",
      "          3310,    13, 18884,   500,    13,  9651,   500,    13,    13,  9651,\n",
      "           849,  1939,  1203,   515,   278,  7797,  2133,  4840, 29892,  1235,\n",
      "         29915, 29879,  1018,   304,  1207,   697,    13,  9651,  4669,  5446,\n",
      "           353, 13341, 29889,  7302, 29898,  3177,   416,    13,  9651,   565,\n",
      "           313,  5415,  1275,  1870, 29897,   426,    13, 18884,  3183,   716,\n",
      "         19014,  5308,  2451,   890,    13,  9651,   500,    13,    13,  9651,\n",
      "           849,  1286,  4078,   372,   304,  8086, 29892,   565,  1950,   322,\n",
      "          1661, 29899,  4304,    13,  9651,   565,   313,  5415, 28919, 18896,\n",
      "         13902, 29897,   426,    13, 18884,   565,   313,  8173,  2283,  2804,\n",
      "          1870, 29897,   426,    13,   462,  1678,  2436,  1523, 13120,  2061,\n",
      "         29898,  8173,  2283, 29892,  5446,   416,    13, 18884,   500,    13,\n",
      "          9651,   500,  1683,   426,    13, 18884, 17927, 29889, 25442,   703,\n",
      "           348, 15550, 13902,  1203,  6571, 13213,   630,   613,  5446,   416,\n",
      "            13,  9651,   500,    13,    13,  9651,   736,  5446, 29936,    13,\n",
      "          4706,   500,    13,    13, 28956,     2]], device='cuda:0')}\n"
     ]
    }
   ],
   "source": [
    "for batch in trainer.get_train_dataloader():\n",
    "    print(batch)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ee4890be-187b-48d6-afb7-8846e0965675",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1/1 00:00, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "('codellama-instruct-repair/tokenizer_config.json',\n",
       " 'codellama-instruct-repair/special_tokens_map.json',\n",
       " 'codellama-instruct-repair/tokenizer.json')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()\n",
    "trainer.save_state()\n",
    "trainer.save_model(output_dir=\"codellama-instruct-repair\")\n",
    "tokenizer.save_pretrained(save_directory=\"codellama-instruct-repair\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "166a33e0-69c4-4016-b601-bd7dac8b9020",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
